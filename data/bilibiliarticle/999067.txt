如果要求你杀掉一个不断哀求的机器人，你会动手吗？即使你知道，这段哀求不过是它的预设程序，它并不会真的感受到疼痛，也不会真的伤害到它？在技术越来越发展的现在，很多机器人相关的伦理问题渐渐进入人们的视线。人们赋予机器人类似的思维逻辑，设计它与人类近似的互动，也同样让我们在面对机器人时，虽然明知道它只是一段设定好的编程，却仍不可避免的沾染了人类的情感。最近，德国一项研究中，要求受试者与一个名叫NAO的机器人合作，回答问卷。但他们真正研究的，是回答结束后，在被要求关掉机器人时，面对NAO的哀求，人们的反应。机器人NAO会哀求，连连说NO，会告诉受试者它害怕黑暗，会恳请他们不要辜负自己对他们的信任。正如预测的一样，参与者在面对NAO的恳求时，犹豫了……所有人都知道，切断电源对于机器人NAO来说没有任何影响。但是，与对照组相比，有三分之二的人犹豫了接近两倍的时间，甚至还有三分之一的人最终拒绝遵守研究人员的指令，留下了NAO。为什么？在面对研究人员的问题时，他们的回答十分简单：“因为NAO说它不想被关掉，如果我强行这样做，我会觉得很抱歉。”但他们没有意识到的是，在这样的回答中，他们完全把NAO当做了人类对待。这个结果其实并没有让人十分意外，也并不是因为这一批受试人员同情心太丰富而造成的偶然事件。其实早在2007年，就有研究人员进行过类似的实验。即使面对这个长得丑丑的机器人，只要它会说话，会表达自己的感情，人们同样会下不去手。甚至连小孩子也不例外。2011年，MIT的研究人员让小孩子倒立拎起芭比娃娃和会哀求的机器猫头鹰毛绒玩具。结果，比起人形的芭比娃娃，小孩子也更不忍心这样对待那个会哭泣着说“我好害怕”的机器玩具。几乎可以说，这样的同情心，是人类的本能。但……人们也不禁思考，人类在面对机器人的时候会有同情心，那反过来呢？耶鲁大学的教授BrianScassellati认为，一个社交技巧娴熟的机器人，能够很轻易的影响我们的观点和行为。——甚至能够阅读我们的情绪，从而加以利用。有点细思恐极了……